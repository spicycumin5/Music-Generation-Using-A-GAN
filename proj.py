# -*- coding: utf-8 -*-
"""Proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOhPcsH6gbAO5H09qAV9MB-v3YKHy3Yl
"""

#Here for Redundancy and testing code
from google.colab import drive
drive.mount('/content/drive')

#THIS CODE WAS TAKEN FROM CHAT GPT(prompt at the bottom of this cell)

from google.colab import drive
import os
import pandas as pd
import shutil
import glob

# Step 1: Mount Google Drive
drive.mount('/content/drive')

# Step 2: Define paths
base_folder = '/content/drive/MyDrive/vgmidi-master/labelled/midi'  # Folder containing files to sort
output_base_folder = '/content/drive/MyDrive/vgmidi-master/SortedMidi'  # Output base folder for sorted files
csv_path = '/content/drive/MyDrive/vgmidi-master/vgmidi_labelled.csv'  # Path to the CSV file

# Step 3: Read the CSV file
df = pd.read_csv(csv_path)

# Step 4: Define groups based on valence and arousal
categories = {
    (1, 1): "Group_1_1",
    (1, -1): "Group_1_-1",
    (-1, 1): "Group_-1_1",
    (-1, -1): "Group_-1_-1"
}

# Create folders for each group
for group in categories.values():
    group_folder = os.path.join(output_base_folder, group)
    os.makedirs(group_folder, exist_ok=True)

# Step 5: Sort files into groups
for _, row in df.iterrows():
    valence = row['valence']  # Sorting parameter 1
    arousal = row['arousal']  # Sorting parameter 2
    piece = row['piece']      # Part of the file name

    # Determine the target group
    category = categories.get((valence, arousal))
    if not category:
        print(f"Invalid valence/arousal combination: {valence}, {arousal}")
        continue

    # Search for files containing the 'piece' in the base folder
    matching_files = glob.glob(os.path.join(base_folder, f"*{piece}*"))
    if not matching_files:
        print(f"No files found for piece: {piece}")
        continue

    # Move matching files to the appropriate group folder
    for file_path in matching_files:
        file_name = os.path.basename(file_path)
        dest_path = os.path.join(output_base_folder, category, file_name)
        shutil.move(file_path, dest_path)
        print(f"Moved: {file_name} to {category}")

print("Sorting completed!")

#PROMPT USED (all lines below this)
#Adjust the code to these parameters. The CSV has multiple collums but only two that are used as sorting parameters.
#They are the valence collum and the arousal collum.
#These collums can ehtier have a value of 1 or -1 and the files are sorted into four groups based on this.
#The piece colum contains parts of the names of the files that need to be sorted, but not the whole name.
#The files being sorted must end up in 4 spereate folders in the end

# Code was generated from Chat GPT (Prompt at bottom)

# Valid emotions
from pathlib import Path # Import the Path object from the pathlib module
valid_emotions = ["happiness", "sadness", "excitement", "calmness"]

# Ask for user input
user_input = ""
while user_input not in valid_emotions:
    print("Which emotion do you want the generated music to be? (happiness, sadness, excitement, calmness):")
    user_input = input().strip().lower()
    if user_input not in valid_emotions:
        print("Invalid input. Please enter one of the following options: happiness, sadness, excitement, calmness.")

# Use the input in an if statement
if user_input == "happiness":
     archive_path = Path("/content/drive/My Drive/vgmidi-master/SortedMidi/Group_1_1")
elif user_input == "sadness":
    archive_path = Path("/content/drive/My Drive/vgmidi-master/SortedMidi/Group_-1_-1")
elif user_input == "excitement":
    archive_path = Path("/content/drive/My Drive/vgmidi-master/SortedMidi/Group_-1_1")
elif user_input == "calmness":
    archive_path = Path("/content/drive/My Drive/vgmidi-master/SortedMidi/Group_1_-1")

print(f"You selected: {user_input}")

#Prompt Used

#now make code that asks the user "which emotion they want the generated music to be:
#happiness, sadness, excitement, or calmness." and checks for them to input one of these options,
#that being happiness, sadness, excitement, and calmness.
#Then it uses that input as part of an if statment for which you can leave the body blank for now but just set the paratmaters for it

import numpy as np
import tensorflow as tf
import numpy
import tensorflow
import music21
import matplotlib

from tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional

from tensorflow.keras.layers import BatchNormalization, LeakyReLU
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from music21 import converter, instrument, note, chord, stream
from pathlib import Path
import matplotlib.pyplot as plt


SEQUENCE_LENGTH = 100
LATENT_DIMENSION = 1000
BATCH_SIZE = 16
EPOCHS = 100
SAMPLE_INTERVAL = 1

def get_notes():
    """ Get all the notes and chords from the midi files """
   # done in previous node by our adjustment
    notes = []

    for file in archive_path.glob("*.mid"):
        midi = converter.parse(file)

        print("Parsing %s" % file)

        notes_to_parse = midi.flat.notes

        for element in notes_to_parse:
            if isinstance(element, note.Note):
                notes.append(str(element.pitch))
            elif isinstance(element, chord.Chord):
                notes.append('.'.join(str(n) for n in element.normalOrder))

    return notes

def prepare_sequences(notes, n_vocab):
    """ Prepare the sequences used by the Neural Network """
    sequence_length = 100

    # Get all pitch names
    pitchnames = sorted(set(item for item in notes))

    # Create a dictionary to map pitches to integers
    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))

    network_input = []
    network_output = []

    # create input sequences and the corresponding outputs
    for i in range(0, len(notes) - sequence_length, 1):
        sequence_in = notes[i:i + sequence_length]
        sequence_out = notes[i + sequence_length]
        network_input.append([note_to_int[char] for char in sequence_in])
        network_output.append(note_to_int[sequence_out])

    n_patterns = len(network_input)

    # Reshape the input into a format compatible with LSTM layers
    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))

    # Normalize input between -1 and 1
    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)
    network_output = to_categorical(network_output, num_classes=n_vocab)  # Use to_categorical from TensorFlow's Keras

    return network_input, network_output  # Add this return statement


def create_midi(prediction_output, filename):
    """ convert the output from the prediction to notes and create a midi file
        from the notes """
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for item in prediction_output:
        pattern = item[0]
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='{}.mid'.format(filename))

class GAN():
    def __init__(self, rows):
        self.seq_length = rows
        self.seq_shape = (self.seq_length, 1)
        self.latent_dim = 1000
        self.disc_loss = []
        self.gen_loss =[]

        optimizer = Adam(0.0002, 0.5)

        # Build and compile the discriminator
        self.discriminator = self.build_discriminator()
        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

        # Build the generator
        self.generator = self.build_generator()

        # The generator takes noise as input and generates note sequences
        z = Input(shape=(self.latent_dim,))
        generated_seq = self.generator(z)

        # For the combined model we will only train the generator
        self.discriminator.trainable = False

        # The discriminator takes generated images as input and determines validity
        validity = self.discriminator(generated_seq)

        # The combined model  (stacked generator and discriminator)
        # Trains the generator to fool the discriminator
        self.combined = Model(z, validity)
        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)

    def build_discriminator(self):
        model = Sequential()
        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))
        model.add(Bidirectional(LSTM(512)))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=0.2))

        # Adding Minibatch Discrimination
        model.add(Dense(100))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.5))
        model.add(Dense(1, activation='sigmoid'))
        model.summary()

        seq = Input(shape=self.seq_shape)
        validity = model(seq)

        return Model(seq, validity)

    def build_generator(self):

        model = Sequential()
        model.add(Dense(256, input_dim=self.latent_dim))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))
        model.add(Reshape(self.seq_shape))
        model.summary()

        noise = Input(shape=(self.latent_dim,))
        seq = model(noise)

        return Model(noise, seq)

    def train(self, epochs, batch_size=128, sample_interval=50):

        # Load and convert the data
        notes = get_notes()
        n_vocab = len(set(notes))
        X_train, y_train = prepare_sequences(notes, n_vocab)

        # Adversarial ground truths
        real = np.ones((batch_size, 1))
        fake = np.zeros((batch_size, 1))

        # Training the model
        for epoch in range(epochs):

            # Training the discriminator
            # Select a random batch of note sequences
            idx = np.random.randint(0, X_train.shape[0], batch_size)
            real_seqs = X_train[idx]

            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))
            #noise = (noise-242)/242
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Generate a batch of new note sequences
            gen_seqs = self.generator.predict(noise)

            # Train the discriminator
            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)
            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)


            #  Training the Generator
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Train the generator (to have the discriminator label samples as real)
            g_loss = self.combined.train_on_batch(noise, real)

            # Print the progress and save into loss lists
            if epoch % sample_interval == 0:
               # print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))
                self.disc_loss.append(d_loss[0])
                self.gen_loss.append(g_loss)

        self.generate(notes)
        self.plot_loss()

    def generate(self, input_notes):
        # Get pitch names and store in a dictionary
        notes = input_notes
        pitchnames = sorted(set(item for item in notes))
        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))

        # Use random noise to generate sequences
        noise = np.random.normal(0, 1, (1, self.latent_dim))
        predictions = self.generator.predict(noise)

        pred_notes = [x*242+242 for x in predictions[0]]

        # Map generated integer indices to note names, with error handling
        pred_notes_mapped = []
        for x in pred_notes:
            index = int(x)
            if index in int_to_note:
                pred_notes_mapped.append(int_to_note[index])
            else:
                # Fallback mechanism: Choose a default note when the index is out of range
                pred_notes_mapped.append('C5')  # You can choose any default note here

        create_midi(pred_notes_mapped, 'gan_final')


    def plot_loss(self):
        plt.plot(self.disc_loss, c='red')
        plt.plot(self.gen_loss, c='blue')
        plt.title("GAN Loss per Epoch")
        plt.legend(['Discriminator', 'Generator'])
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)
        plt.close()

if __name__ == '__main__':
    gan = GAN(rows=SEQUENCE_LENGTH)
    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)

    # Save the generator and discriminator models
    gan.generator.save("generator_model.h5")
    gan.discriminator.save("discriminator_model.h5")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from music21 import stream, note, chord, instrument
#from create_generator_model import get_notes, LATENT_DIMENSION

instr = instrument.Violin()

def create_midi(prediction_output, filename):
    """ convert the output from the prediction to notes and create a midi file
        from the notes """
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for item in prediction_output:
        pattern = item[0]
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instr
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instr
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='{}.mid'.format(filename))


def generate_music(generator_model, latent_dim, n_vocab, length=500):
    """ Generate new music using the trained generator model """
    # Create random noise as input to the generator
    noise = np.random.normal(0, 1, (1, latent_dim))
    predictions = generator_model.predict(noise)

    # Scale back the predictions to the original range
    pred_notes = [x * (n_vocab / 2) + (n_vocab / 2) for x in predictions[0]]

    # Map generated integer indices to note names
    pitchnames = sorted(set(item for item in notes))
    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
    pred_notes_mapped = [int_to_note[int(x)] for x in pred_notes]

    return pred_notes_mapped[:length]

if __name__ == '__main__':
    # Load the trained generator model
    generator_model = load_model("generator_model.h5")

    # Load the processed notes and get the number of unique pitches
    notes = get_notes()
    n_vocab = len(set(notes))

    # Generate new music sequence
    generated_music = generate_music(generator_model, LATENT_DIMENSION, n_vocab)

    # Create a MIDI file from the generated music
    create_midi(generated_music, 'generated_music')

# ipython-input-13-9aa333785b39
from google.colab import files

def download_midi(filename):
    """Downloads the generated MIDI file to your local machine."""
    files.download(filename)

if __name__ == "__main__":
    midi_file_path = "generated_music.mid"
    download_midi(midi_file_path)
    print(f"MIDI file '{midi_file_path}' downloaded. Please play it on your local machine.")